Cross-Lingual Summarization with Transfer Learning & In-Context Learning

This repository contains the code and experiments for the paper:
â€œExploring Cross-Lingual Transfer Learning Techniques for Abstractive Text Summarizationâ€
by Darpan Aswal, UniversitÃ© Paris-Saclay.

We investigate whether In-Context Learning (ICL) is an inherent property of transformer models or an emergent capability of large language models (LLMs), with a focus on cross-lingual abstractive summarization.

We compare Google mT5-xl, Facebook mBART50, and GPT-4o-mini across zero-shot, one-shot, and two-shot summarization tasks on the WikiLingua dataset.

â¸»

ğŸš€ Key Features
	â€¢	Models Supported
	â€¢	mT5-xl (fine-tuned with QLoRA in 4-bit)
	â€¢	mBART50
	â€¢	Llama-3.2-1B-Instruct (for baseline comparison)
	â€¢	Experiments
	â€¢	Multilingual summarization: English â†’ English, French â†’ French
	â€¢	Cross-lingual summarization: French â†’ English
	â€¢	Zero-shot, 1-shot, and 2-shot settings
	â€¢	Fine-tuning
	â€¢	QLoRA fine-tuning for mT5
	â€¢	Standard fine-tuning for mBART50
	â€¢	Evaluation Metrics
	â€¢	ROUGE-1, ROUGE-2, ROUGE-L
	â€¢	BERTScore (F1)
	â€¢	Reproducible Pipelines
	â€¢	Dataset preprocessing and sampling
	â€¢	Fine-tuning scripts with Hugging Face integration
	â€¢	Automated evaluation and logging of metrics

â¸»

ğŸ“‚ Repository Structure

â”œâ”€â”€ download_model.py        # Utilities to load Hugging Face models
â”œâ”€â”€ download_datasets.py     # Prepares WikiLingua dataset (EN, FR, FRâ†’EN)
â”œâ”€â”€ finetune.py              # Fine-tuning pipeline (QLoRA for mT5, standard for mBART50)
â”œâ”€â”€ load_finetuned.py        # Load fine-tuned models from Hugging Face Hub
â”œâ”€â”€ metrics.py               # ROUGE + BERTScore evaluation
â”œâ”€â”€ main.py                  # Run experiments (zero-shot, 1-shot, 2-shot)
â”œâ”€â”€ datasets/                # Preprocessed CSV datasets
â”‚   â”œâ”€â”€ train.csv / val.csv / test.csv
â”‚   â”œâ”€â”€ train_fr.csv / ...
â”‚   â””â”€â”€ train_cross.csv / ...
â””â”€â”€ rouge_results.csv        # Evaluation results are stored here


â¸»

ğŸ“Š Results (Summary)
	â€¢	Fine-tuning improves mBART50 and mT5 performance, but cross-lingual generalization remains weak.
	â€¢	mBART50-English outperforms mBART50-Multilingual when evaluated on English.
	â€¢	GPT-4o-mini shows strong zero-shot performance and benefits from 1-shot prompts, but gains plateau with 2-shot prompting.
	â€¢	ICL is not inherent to transformers, but emerges in modern LLMs.

â¸»

âš¡ï¸ Quickstart

1ï¸âƒ£ Install dependencies

pip install -r requirements.txt

2ï¸âƒ£ Download datasets

python download_datasets.py

3ï¸âƒ£ Fine-tune a model

Example: Fine-tune mT5 on English summarization

python finetune.py --model mT5 --finetune_type english

Fine-tuned models will be pushed to your Hugging Face Hub.

4ï¸âƒ£ Run experiments

Zero-shot evaluation with mBART50 on multilingual data:

python main.py --model mBART50 --experiment zero-shot --finetune_type multilingual --num_examples 100

1-shot evaluation with mT5 on cross-lingual data:

python main.py --model mT5 --experiment 1-shot --finetune_type crosslingual


â¸»

ğŸ“ˆ Evaluation

Scores are saved to rouge_results.csv in the format:

model_name, experiment_type, dataset_name, ROUGE-1, ROUGE-2, ROUGE-L, BERT-F1

You can analyze results with:

import pandas as pd
df = pd.read_csv("rouge_results.csv")
print(df.groupby(["model_name", "experiment_type"]).mean())


â¸»

ğŸ¤ Contributing

Pull requests are welcome! Please open an issue first for discussions on major changes.

â¸»

ğŸ“œ Citation

If you use this code, please cite:

@article{aswal2025crosslingual,
  title={Exploring Cross Lingual Transfer Learning Techniques for Abstractive Text Summarization},
  author={Aswal, Darpan},
  journal={Preprint},
  year={2025}
}


â¸»

ğŸŒŸ Acknowledgments
	â€¢	Hugging Face Transformers
	â€¢	WikiLingua Dataset
	â€¢	Inspiration from recent research on ICL emergence in LLMs

â¸»

ğŸ‘‰ This README is written to be developer-friendly while still highlighting the research contributions.

Would you like me to also create a visual diagram (pipeline illustration in Markdown/mermaid) for the README so people can quickly grasp your experiment workflow?
